{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Deep Q-Networks</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this chapter, we'll talk about problems with the Value iteration method and introduce its variation, called __Q-learning__. In particular, we'll look at the application of Q-learning to so-called \"grid world\" environments, which is called __tabular Q-learning__, and then we'll discuss Q-learning in conjunction with neural networks. This combination has the name __DQN__. At the end of the chapter, we'll reimplement a DQN algorithm from the famous paper, *Playing Atari with Deep Reinforcement Learning by V. Mnih and others, published in 2013*, which started a new era in RL development.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=\"700px\" src=\"./assets/dqn.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 01. Real-Life Value Iteration\n",
    "\n",
    "---\n",
    "\n",
    "Let's recall the value iteration method. In every step, loop over all states, and for every state, perform an update of its value with a Bellman approximation. The other variation of this method for Q-values (values for actions) is almost the same. However, in here we approximate and store values for every state and action.\n",
    "\n",
    "Despite the impovement that the value iteration method has over Cross-Entropy method, it has its own limitations. For example:\n",
    "1. __Count of states and ability to loop over them:__ <br> In Value iteration method we assume we know all states in advance and we can iterate over them and store the approximation of the value state. This is only possible with simple environments and not with the complex ones. <br><br>\n",
    "\n",
    "2. __Limited to discrete action sapce only:__ <br> The value iteration approach limits us to discrete action spaces and we cannot use this approch for continuous control problems (where actions can represent continuous variables) such as the angle of a steering wheel, the force on an actuator, or the temperature of a heater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 02. Tabular Q-Learning\n",
    "\n",
    "---\n",
    "\n",
    "There is not need to iterate over every state in the state space. We have an environment that can be used as a source of real-life samples of states. If some state in the state space is not shown to us by the environment, why should we care about its value? We can use states obtained from the environment to update values of states, which can save us lots of work. This modification of the Value iteration method is known as __Q-learning__, and for cases with explicit state-to-value mappings, has the following steps:\n",
    "\n",
    "1. Start with an empty table, mapping states to values of actions.\n",
    "2. By interacting with the environment, obtain the tuple s, a, r, s′ (state, action, reward, and the new state). In this step, we need to decide which action to take with consideration of exploration vs. exploitation.\n",
    "3. Update the Q(s, a) value using the Bellman approximation:\n",
    "<img width=\"300px\" src=\"assets/equ_1.png\">\n",
    "4. Repeat from step 2.\n",
    "\n",
    "As in Value iteration, the end condition could be some threshold of the update or we can perform test episodes to estimate the expected reward from the policy. \n",
    "\n",
    "<br><center>***</center><br>\n",
    "\n",
    "We update the Q-values or Q(s, a) using a __\"blending\"__ technique, which is the average between old and new values of Q using learning rate α (with a value from 0 to 1):\n",
    "\n",
    "<img width=\"480px\" src=\"assets/equ2.png\">\n",
    "\n",
    "This allows values of Q to converge smoothly, even if our environment is noisy.\n",
    "\n",
    "<br><center>***</center><br>\n",
    "So let's take a look at the final version of the algorithm:\n",
    "1. Start with an empty table for Q(s, a).\n",
    "2. Obtain (s, a, r, s′) from the environment.\n",
    "3. Make a Bellman update:\n",
    "<img width=\"480px\" src=\"assets/equ3.png\">\n",
    "4. Check convergence conditions. If not met, repeat from step 2.\n",
    "\n",
    "As mentioned earlier, this method is called __tabular Q-learning__, as we keep a table of states with their Q-values. Now let's try it on our FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2         # Learning rate \n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent class\n",
    "class Agent:\n",
    "    \n",
    "    \n",
    "    # The constructor class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # The environment\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        \n",
    "        # The initial state\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        # Initialize an empty table for Q(s, a)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "        \n",
    "    # Function for obtaining the next transition from the environment\n",
    "    def sample_env(self):\n",
    "\n",
    "        # Take a random action\n",
    "        action = self.env.action_space.sample()\n",
    "\n",
    "        # Update the old state with current state\n",
    "        old_state = self.state\n",
    "\n",
    "        # Take the given action and get the observation (next state), reward, done, info\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "\n",
    "        # Update the current state\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "        # Return S, A, R, S'\n",
    "        return (old_state, action, reward, new_state)\n",
    "\n",
    "    \n",
    "    # Function for finding the best action to take from given state\n",
    "    def best_value_and_action(self, state):\n",
    "\n",
    "        # Initialize the best action value and best action\n",
    "        best_action_value, best_action = None, None\n",
    "\n",
    "        # Iterate through action space\n",
    "        for action in range(self.env.action_space.n):\n",
    "\n",
    "            # Fetch the action value\n",
    "            action_value = self.values[(state, action)]\n",
    "\n",
    "            # If the best action value is none OR it's less than action value\n",
    "            if (best_action_value is None) or (best_action_value < action_value):\n",
    "\n",
    "                # Update the best action value\n",
    "                best_action_value = action_value\n",
    "\n",
    "                # Update the best action\n",
    "                best_action = action\n",
    "\n",
    "        return best_action_value, best_action\n",
    "\n",
    "    \n",
    "    # Function for updating the action value table\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "\n",
    "        # Find the best action value to take\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "\n",
    "        # Calculate the new action value using Bellman approximation \n",
    "        new_val = r + GAMMA * best_v\n",
    "\n",
    "        # Fetch the old action value\n",
    "        old_val = self.values[(s, a)]\n",
    "\n",
    "        # Blend the old and new action value (using learning rate) and update the values table\n",
    "        self.values[(s, a)] = old_val * (1 - ALPHA) + new_val * ALPHA\n",
    "\n",
    "        \n",
    "    # Function for playing one full eopisode\n",
    "    def play_episode(self, env):\n",
    "\n",
    "        # Initialize the total reward\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # Get the initial state\n",
    "        state = env.reset()\n",
    "\n",
    "        # Infinite loop\n",
    "        while True:\n",
    "\n",
    "            # Get the best action\n",
    "            _, action = self.best_value_and_action(state)\n",
    "\n",
    "            # Take the given action and get the observation (next state), reward, done, info\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # Add the reward to total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # If terminal state\n",
    "            if is_done:\n",
    "\n",
    "                # Break the loop\n",
    "                break\n",
    "\n",
    "            # Update the current state to next state\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.150\n",
      "Best reward updated 0.150 -> 0.200\n",
      "Best reward updated 0.200 -> 0.250\n",
      "Best reward updated 0.250 -> 0.300\n",
      "Best reward updated 0.300 -> 0.450\n",
      "Best reward updated 0.450 -> 0.550\n",
      "Best reward updated 0.550 -> 0.900\n",
      "Solved in 2398 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Execute the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create the environment\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = Agent()\n",
    "    \n",
    "    # Summary writer for tensorboard\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    # Initialize the iteration number\n",
    "    iter_no = 0\n",
    "    \n",
    "    # Initialize the best reward\n",
    "    best_reward = 0.0\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Increment the iteration number\n",
    "        iter_no += 1\n",
    "        \n",
    "        # Obtain the next transition from the environment\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        \n",
    "        # Update the action value table\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "\n",
    "        # Initialize the reward\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Iterate through TEST_EPISODES numbers\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            \n",
    "            # Update the reward by playing one full episode\n",
    "            reward += agent.play_episode(test_env)\n",
    "            \n",
    "        # Divide reward by TEST_EPISODES\n",
    "        reward /= TEST_EPISODES\n",
    "        \n",
    "        # Add the reward value into tensorboard\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        \n",
    "        # If reward is higher than best reward\n",
    "        if reward > best_reward:\n",
    "            \n",
    "            # Print best reward and reward\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            \n",
    "            # Update the best reward to reward\n",
    "            best_reward = reward\n",
    "            \n",
    "        # If reward is higher than 0.8 then it is SOLVED \n",
    "        if reward > 0.80:\n",
    "            \n",
    "            # Print the iteration number which is solved\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            \n",
    "            # break the loop\n",
    "            break\n",
    "            \n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that this version used more iterations to solve the problem compared to the value iteration method from the previous chapter. The reason for that is that we're no longer using the experience obtained during testing. (In Chapter05/02_frozenlake_q_iteration.py, periodical tests cause an update of Q-table statistics. Here we don't touch Q-values during the test, which cause more iterations before the environment gets solved.) Overall, the total amount of samples required from the environment is almost the same. The reward chart in TensorBoard also shows good training dynamics, which are very similar to the value iteration method.\n",
    "\n",
    "<img width=\"350px\" src=\"./assets/dynfrozen.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 03. Deep Q-learning\n",
    "\n",
    "---\n",
    "\n",
    "The Q-learning method that we've just seen solves the issue with iteration over the full set of states, but still can struggle with situations when the count of the observable set of states is very large. For example, Atari games can have a large variety of different screens, so if we decide to use raw pixels as individual states, we'll quickly realize that we have too many states to track and approximate values for.\n",
    "\n",
    "In some environments, the count of different observable states could be almost infinite. For example, in CartPole the state given to us by the environment is four floating point numbers. The number of combinations of values is finite (they're represented as bits), but this number is extremely large. We could create some bins to discretize those values, but this often creates more problems than it solves: we would need to decide what ranges of parameters are important to distinguish as different states and what ranges could be clustered together.\n",
    "\n",
    "In the case of Atari, one single pixel change doesn't make much difference, so it's efficient to treat both images as a single state. However, we still need to distinguish some of the states. The following image shows two different situations in a game of Pong. We're playing against the AI opponent by controlling a paddle (our paddle is on the right and has a green color, whereas our opponent's is light brown and on the left). The objective of the game is to get the bounding ball past our opponent's paddle, while preventing it from getting past our paddle. The two situations can be considered to be completely different: in the right-hand situation, the ball is close to the opponent, so we can relax and watch. However, the situation on the left is more demanding: assuming that the ball is moving from left to right, which means that the ball is moving toward our side, so we need to move our paddle quickly to avoid losing a point. The situations below are just two from the 10<sup>70802</sup> possible situations, but we want our agent to act on them differently.\n",
    "\n",
    "<img width=\"400px\" src=\"./assets/pongenv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a solution to this problem, we can use a nonlinear representation that maps both state and action onto a value. In machine learning this is called a \"regression problem.\" The concrete way to represent and train such a representation can vary, but, as you may have already guessed from this section's title, using a deep neural network is one of the most popular options, especially when dealing with observations represented as screen images. With this in mind, let's make modifications to the Q-learning algorithm:\n",
    "1. Initialize Q(s, a) with some initial approximation\n",
    "2. By interacting with the environment, obtain the tuple (s, a, r, s′)\n",
    "3. Calculate loss. \n",
    "    - If episode has ended then:\n",
    "$$ L = (Q_{s,a} − r)^2 $$\n",
    "    <br>\n",
    "    - Or otherwise:\n",
    "\n",
    "<img width=\"300px\" src=\"assets/equ4.png\"> \n",
    "\n",
    "4. Update Q(s, a) using the __stochastic gradient descent (SGD)__ algorithm, by minimizing the loss with respect to the model parameters\n",
    "5. Repeat from step 2 until converged\n",
    "\n",
    "The preceding algorithm looks simple but, unfortunately, it won't work very well. Let's discuss what could go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.1. Interaction With the Environment\n",
    "\n",
    "First of all, we need to interact with the environment somehow to receive data to train on. In simple environments, such as FrozenLake, we can act randomly, but is this the best strategy to use? Imagine the game of Pong. What's the probability of winning a single point by randomly moving the paddle? It's not zero but it's extremely small, which just means that we'll need to wait for a very long time for such a rare situation. As an alternative, we can use our Q function approximation as a source of behavior (as we did before in the value iteration method, when we remembered our experience during testing).\n",
    "\n",
    "If our representation of Q is good, then the experience that we get from the environment will show the agent relevant data to train on. However, we're in trouble when our approximation is not perfect (at the beginning of the training, for example). In such a case, our agent can be stuck with bad actions for some states without ever trying to behave differently. This exploration versus exploitation dilemma was mentioned briefly in Chapter 1, What is Reinforcement Learning?. On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently: we shouldn't waste time by randomly trying actions we've already tried and have learned their outcomes. As you can see, random behavior is better at the beginning of the training when our Q approximation is bad, as it gives us more uniformly distributed information about the environment states. As our training progresses, random behavior becomes inefficient and we want to fall back to our Q approximation to decide how to act.\n",
    "\n",
    "A method which performs such a mix of two extreme behaviors is known as an __epsilon-greedy__ method, which just means switching between random and Q policy using the probability hyperparameter ε. By varying ε we can select the ratio of random actions. The usual practice is to start with ε = 1.0 (100% random actions) and slowly decrease it to some small value such as 5% or 2% of random actions. Using an epsilon-greedy method helps both to explore the environment in the beginning and to stick to good policy at the end of the training. There are other solutions to the \"exploration versus exploitation\" problem, and we'll discuss some of them in part three of the book. This problem is one of the fundamental open questions in RL and an active area of research, which is not even close to being resolved completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.2. SGD Optimization\n",
    "\n",
    "The core of our Q-learning procedure is borrowed from the supervised learning. Indeed, we are trying to approximate a complex, nonlinear function Q(s, a) with a neural network. To do this, we calculate targets for this function using the Bellman equation and then pretend that we have a supervised learning problem at hand. That's okay, but one of the fundamental requirements for SGD optimization is that the training data is __independent and identically distributed__ (frequently abbreviated as __i.i.d__).\n",
    "\n",
    "In our case, data that we're going to use for the SGD update doesn't fulfill these criteria:\n",
    "1. Our samples are not independent. Even if we accumulate a large batch of data samples, they all will be very close to each other, as they belong to the same episode.\n",
    "2. Distribution of our training data won't be identical to samples provided by the optimal policy that we want to learn. Data that we have is a result of some other policy (our current policy, random, or both in the case of ε-greedy), but we don't want to learn how to play randomly: we want an optimal policy with the best reward.\n",
    "\n",
    "To deal with this nuisance, we usually need to use a large buffer of our past experience and sample training data from it, instead of using our latest experience. This method is called __replay buffer__. The simplest implementation is a buffer of fixed size, with new data added to the end of the buffer so that it pushes the oldest experience out of it. Replay buffer allows us to train on more-or-less independent data, but data will still be fresh enough to train on samples generated by our recent policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.3. Correlation Between Steps\n",
    "\n",
    "Another practical issue with the default training procedure is also related to the lack of i.i.d in our data, but in a slightly different manner. The Bellman equation provides us with the value of Q(s, a) via Q(s′, a′) (which has the name of bootstrapping). However, both states s and s′ have only one step between them. This makes them very similar and it's really hard for neural networks to distinguish between them. When we perform an update of our network's parameters, to make Q(s, a) closer to the desired result, we indirectly can alter the value produced for Q(s′, a′) and other states nearby. This can make our training really unstable, like chasing our own tail: when we update Q for state s, then on subsequent states we discover that Q(s′, a′) becomes worse, but attempts to update it can spoil our Q(s, a) approximation, and so on.\n",
    "\n",
    "To make training more stable, there is a trick, called __target network__, when we keep a copy of our network and use it for the Q(s′, a′) value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in N steps (where N is usually quite a large hyperparameter, such as 1k or 10k training iterations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.4. The Markov Property\n",
    "\n",
    "Our RL methods use MDP formalism as their basis, which assumes that the environment obeys the Markov property: observation from the environment is all that we need to act optimally (in other words, our observations allow us\n",
    "to distinguish states from one another). As we've seen on the preceding Pong's screenshot, one single image from the Atari game is not enough to capture all important information (using only one image we have no idea about the speed and direction of objects, like the ball and our opponent's paddle). This obviously violates the Markov property and moves our single-frame Pong environment into the area of __partially observable MDPs (POMDP)__. A POMDP is basically MDP without the Markov property and they are very important in practice. For example, for most card games where you don't see your opponents' cards, game observations are POMDPs, because current observation (your cards and cards on the table) could correspond to different cards in your opponents' hands.\n",
    "\n",
    "We'll not discuss POMPDs in detail in this book, so, for now, we'll use a small technique to push our environment back into the MDP domain. The solution is maintaining several observations from the past and using them as a state. In the case of Atari games, we usually stack k subsequent frames together and use them as the observation at every state. This allows our agent to deduct the dynamics of the current state, for instance, to get the speed of the ball and its direction. The usual \"classical\" number of k for Atari is four. Of course, it's just a hack, as there can be longer dependencies in the environment, but for most of the games it works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.5. The Final Form of DQN Training\n",
    "\n",
    "There are many more tips and tricks that researchers have discovered to make DQN training more stable and efficient, and we'll cover the best of them in the next chapter. However, ε-greedy, replay buffer, and target network form? the basis that allows DeepMind to successfully train a DQN on a set of 49 Atari games and demonstrate the efficiency of this approach applied to complicated environments.\n",
    "\n",
    "The original paper (without target network) was published at the end of 2013 (Playing Atari with Deep Reinforcement Learning 1312.5602v1, Mnih and others.), and they used seven games for testing. Later, at the beginning of 2015, a revised version of the article, with 49 different games, was published in Nature (Human-Level Control Through Deep Reinforcement Learning doi:10.1038/nature14236, Mnih and others.)\n",
    "\n",
    "The algorithm for DQN from the preceding papers has the following steps:\n",
    "1. Initialize parameters for Q(s, a) and Q&#x0302;(s, a) with random weights, ε←1.0, and empty replay buffer\n",
    "2. With probability ε, select a random action a, otherwise a = argmax<sub>a</sub>Q<sub>s,a</sub>\n",
    "3. Execute action a in an emulator and observe reward r and the next state s′\n",
    "4. Store transition (s, a, r, s′) in the replay buffer\n",
    "5. Sample a random minibatch of transitions from the replay buffer\n",
    "6. For every transition in the buffer, calculate target y = r if the episode has ended at this step or otherwise:\n",
    "<img width=\"250px\" src=\"assets/equ5.png\"> \n",
    "7. Calculate loss:\n",
    "<img width=\"145px\" src=\"assets/equ6.png\"> \n",
    "8. Update Q(s, a) using the SGD algorithm by minimizing the loss in respect to model parameters\n",
    "9. Every N steps copy weights from Q to Q&#x0302;\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "Let's implement it now and try to beat some of the Atari games!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 04. DQN on Pong\n",
    "\n",
    "---\n",
    "\n",
    "Before we jump into the code, some introduction is needed. Our examples are becoming increasingly challenging and complex, which is not surprising, as the complexity of problems we're trying to tackle is also growing. The examples are as simple and concise as possible, but some of the code may be difficult to understand at first.\n",
    "\n",
    "Another thing to note is performance. Our previous examples for FrozenLake, or CartPole, were not demanding from a performance perspective, as observations were small, neural network parameters were tiny, and shaving off extra milliseconds in the training loop wasn't important. However, from now on, that's not the case anymore. One single observation from the Atari environment is 100k values, which has to be rescaled, converted to floats, and stored in the replay buffer. One extra copy of this data array can cost you training speed, which is not seconds and minutes anymore, but could be hours even on the fastest GPU available. The neural network training loop could also be a bottleneck. Of course, RL models are not such huge monsters as state-of-the-art ImageNet models, but even the DQN model from 2015 has more than 1.5M parameters, which is a lot for a GPU to crunch. So, to make a long story short: performance matters, especially when you're experimenting with hyperparameters and need to wait not for a single model to train, but for dozens of them.\n",
    "\n",
    "PyTorch is quite expressive, so more-or-less efficient processing code could look much less cryptic than optimized TensorFlow graphs, but there is still lots of opportunity for doing things slowly and making mistakes. For example, a naive version of DQN loss computation, which loops over every batch sample, is about two times slower than a parallel version. However, a single extra copy of the data batch can make the speed of the same code 13 times slower, which is quite significant.\n",
    "\n",
    "This example has been split into three modules due to its length, logical structure, and reusability. The modules are as follows:\n",
    "\n",
    "- __Chapter06/lib/wrappers.py:__ These are Atari environment wrappers mostly taken from the OpenAI Baselines project\n",
    "- __Chapter06/lib/dqn_model.py:__ This is the DQN neural net layer, with the same architecture as the DeepMind DQN from the Nature paper\n",
    "- __Chapter06/02_dqn_pong.py:__ This is the main module with the training loop, loss function calculation, and experience replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.1. Wrappers\n",
    "\n",
    "Tackling Atari games with RL is quite demanding from a resource perspective. To make things faster, several transformations are applied to the Atari platform interaction, which are described in DeepMind's paper. Some of these transformations influence only performance, but some address Atari platform features that make learning long and unstable. Transformations are usually implemented as OpenAI Gym wrappers of various kinds. The full list is quite lengthy and there are several implementations of the same wrappers in various sources. My personal favorite is in the OpenAI repository called baselines, which is a set of RL methods and algorithms implemented in TensorFlow and applied to popular benchmarks, to establish the common ground for comparing methods. The repository is available from https://github.com/openai/baselines, and wrappers are available in this file: https://github.com/openai/baselines/blob/master/baselines/common/atari_ wrappers.py.\n",
    "\n",
    "The full list of Atari transformations used by RL researchers includes:\n",
    "\n",
    "- Converting individual lives in the game into separate episodes. In general, an episode contains all the steps from the beginning of the game until the \"Game over\" screen appears?, which can last for thousands of game steps (observations and actions). Usually, in arcade games, the player is given several lives, which provide several attempts in the game. This transformation splits a full episode into individual small episodes for every life that a player has. Not all games support this feature (for example, Pong doesn't), but for the supported environments, it usually helps to speed up convergence as our episodes become shorter.\n",
    "- In the beginning of the game, performing a random amount (up to 30) of no-op actions. This should stabilize training, but there is no proper explanation why it is the case.\n",
    "- Making an action decision every K steps, where K is usually 4 or 3. On intermediate frames, the chosen action is simply repeated. This allows training to speed up significantly, as processing every frame with a neural network is quite a demanding operation, but the difference between consequent frames is usually minor.\n",
    "- Taking the maximum of every pixel in the last two frames and using it as an observation. Some Atari games have a flickering effect, which is due to the platform's limitation (Atari has a limited amount of sprites that can be shown on a single frame). For a human eye, such quick changes are not visible, but they can confuse neural networks.\n",
    "- Pressing FIRE in the beginning of the game. Some games (including Pong and Breakout) require a user to press the FIRE button to start the game. In theory, it's possible for a neural network to learn to press FIRE itself, but it will require much more episodes to be played. So, we press FIRE in the wrapper.\n",
    "- Scaling every frame down from 210 × 160, with three color frames, into a single-color 84 × 84 image. Different approaches are possible. For example, the DeepMind paper describes this transformation as taking the Y-color channel from the YCbCr color space and then rescaling the full image to an 84 × 84 resolution. Some other researchers do grayscale transformation, cropping non-relevant parts of the image and then scaling down. In the Baselines repository (and in the following example code), the latter approach is used.\n",
    "- Stacking several (usually four) subsequent frames together to give the network the information about the dynamics of the game's objects.\n",
    "- Clipping the reward to −1, 0, and 1 values. The obtained score can vary wildly among the games. For example, in Pong you get a score of 1 for every ball that your opponent passes behind you. However, in some games, like KungFu, you get a reward of 100 for every enemy killed. This spread in reward values makes our loss have completely different scales between the games, which makes it harder to find common hyperparameters for a set of games. To fix this, reward just gets clipped to the range [−1...1].\n",
    "- Converting observations from unsigned bytes to float32 values. The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for a neural network. So, we need to convert the image into floats and rescale the values to the range [0.0...1.0].\n",
    "\n",
    "In our example on Pong, we don't need some of the above wrappers, such as converting lives into separate episodes and reward clipping, so those wrappers aren't included in the example code. However, you should be aware of them, just\n",
    "in case you decide to experiment with other games. Sometimes, when the DQN is not converging, the problem is not in the code but in the wrongly wrapped environment. I've spend several days debugging convergence issues caused by missing the __FIRE__ button press at the beginning of a game!\n",
    "\n",
    "Let's take a look at the implementation of individual wrappers from Chapter06/lib/wrappers.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import collections\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "    def step(self, action):\n",
    "        return self.env.step.action()\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding wrapper presses the __FIRE__ button in environments that require them for the game to start. In addition to pressing __FIRE__, this wrapper checks for several corner cases that are present in some games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every 'skip'-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper combines the repetition of actions during K frames and pixels from two consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this wrapper is to convert input observations from the emulator, which normally has a resolution of 210 × 160 pixels with RGB color channels, to a grayscale 84 × 84 image. It does this using a colorimetric grayscale conversion (which is closer to human color perception than a simple averaging of color channels), resizing the image and cropping the top and bottom parts of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "        old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class creates a stack of subsequent frames along the first dimension and returns them as an observation. The purpose is to give the network an idea about the dynamics of the objects, such as the speed and direction of the ball in Pong or how enemies are moving. This is very important information, which it is not possible to obtain from a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple wrapper changes the shape of the observation from HWC to the CHW format required by PyTorch. The input shape of the tensor has a color channel as the last dimension, but PyTorch's convolution layers assume the color channel to be the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final wrapper we have in the library converts observation data from bytes to floats and scales every pixel's value to the range [0.0...1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def make_env(env_name):\n",
    "        env = gym.make(env_name)\n",
    "        env = MaxAndSkipEnv(env)\n",
    "        env = FireResetEnv(env)\n",
    "        env = ProcessFrame84(env)\n",
    "        env = ImageToPyTorch(env)\n",
    "        env = BufferWrapper(env, 4)\n",
    "        return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the file is a simple function that creates an environment by its name and applies all the required wrappers to it. That's it for wrappers, so let's look at our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.2. DQN Model\n",
    "\n",
    "The model published in Nature has three convolution layers followed by two fully connected layers. All layers are separated by ReLU nonlinearities. The output of the model is Q-values for every action available in the environment, without nonlinearity applied (as Q-values can have any value). The approach to have all Q-values calculated with one pass through the network helps us to increase speed significantly in comparison to treating Q(s, a) literally and feeding observations and actions to the network to obtain the value of the action.\n",
    "\n",
    "The code of the model is in Chapter06/lib/dqn_model.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class DQN(nn.Module):\n",
    "def __init__(self, input_shape, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    self.conv = nn.Sequential(nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), \n",
    "                              nn.ReLU(), \n",
    "                              nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "                              nn.ReLU())\n",
    "    conv_out_size = self._get_conv_out(input_shape)\n",
    "    self.fc = nn.Sequential(nn.Linear(conv_out_size, 512),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(512, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to write our network in the generic way, it was implemented in two parts: convolution and sequential. PyTorch doesn't have a 'flatter' layer which could transform a 3D tensor into a 1D vector of numbers, required to feed convolution output to the fully connected layer. This problem is solved in the forward() function, where we can reshape our batch of 3D tensors into a batch of 1D vectors.\n",
    "\n",
    "Another small problem is that we don't know the exact number of values in the output from the convolution layer produced with input of the given shape. However, we need to pass this number to the first fully connected layer constructor. One possible solution would be to hard-code this number, which is a function of input shape (for 84 × 84 input, the output from the convolution layer will have 3136 values), but it's not the best way, as our code becomes less robust to input shape change. The better solution would be to have a simple function (\\_get_conv_out()) that accepts the input shape and applies the convolution layer to a fake tensor of such a shape. The result of the function will be equal to the number of parameters returned by this application. It will be fast, as this call will be done once on model creation but will allow us to have generic code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece of the model is the forward() function, which accepts the 4D input tensor (the first dimension is batch size, the second is the color channel, which is our stack of subsequent frames, while the third and fourth are image dimensions). The application of transformations is done in two steps: first we apply the convolution layer to the input and then we obtain a 4D tensor on output. This result is flattened to have two dimensions: a batch size and all the parameters returned by the convolution for this batch entry as one long vector of numbers. This is done by the view() function of the tensors, which lets one single dimension be a -1 argument as a wildcard for the rest of the parameters. For example, if we have a tensor T of shape (2, 3, 4), which is a 3D tensor of 24 elements, we can reshape it into a 2D tensor with six rows and four columns using T.view(6, 4). This operation doesn't create a new memory object or move the data in memory, it just changes the higher-level shape of the tensor. The same result could be obtained by T.view(-1, 4) or T.view(6, -1), which is very convenient when your tensor has a batch size in the first dimension. Finally, we pass this flattened 2D tensor to our fully connected layers to obtain Q-values for every batch input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.3. Training\n",
    "\n",
    "The third module contains the experience replay buffer, the agent, the loss function calculation, and the training loop itself. Before going into the code, something needs to be said about the training hyperparameters. DeepMind's Nature paper contained a table with all the details about hyperparameters used to train its model on all\n",
    "49 Atari games used for evaluation. DeepMind kept all those parameters the same for all games (but trained individual models for every game), and it was the team's intention to show that the method is robust enough to solve lots of games with varying complexity, action space, reward structure, and other details using one single model architecture and hyperparameters. However, our goal here is much more modest: we want to solve just the Pong game.\n",
    "\n",
    "Pong is quite simple and straightforward in comparison to other games in the Atari test set, so the hyperparameters in the paper are overkill for our task. For example, to get the best result on all 49 games, DeepMind used a million-observations replay buffer, which requires approximately 20 GB of RAM to keep and lots of samples from the environment to populate. The epsilon decay schedule that was used is also not the best for a single Pong game. In the training, DeepMind linearly decayed epsilon from 1.0 to 0.1 during the first million frames obtained from the environment. However, my own experiments have shown that for Pong, it's enough to decay epsilon over the first 100k frames and then keep it stable. The replay buffer can also be much smaller: 10k transitions will be enough. In the following example, I've used my parameters. These differ from the parameters in the paper but allow us to solve Pong about ten times faster. On a GeForce GTX 1080 Ti, the following version converges to a mean score of 19.5 in one to two hours, but with DeepMind's hyperparameters it will require at least a day.\n",
    "\n",
    "This speed up, of course, is fine-tuning for one particular environment and can break convergence on other games. You're free to play with the options and other games from the Atari set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wrappers\n",
    "from lib import dqn_model\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import required modules and define hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two values set the default environment to train on and the reward boundary for the last 100 episodes to stop training. They are just defaults; you can redefine them using the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "REPLAY_START_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters define the following:\n",
    "- Our gamma value used for Bellman approximation\n",
    "- The batch size sampled from the replay buffer (BATCH_SIZE)\n",
    "- The maximum capacity of the buffer (REPLAY_SIZE)\n",
    "- The count of frames we wait for before starting training to populate the replay buffer (REPLAY_START_SIZE)\n",
    "- The learning rate used in the Adam optimizer, which is used in this example\n",
    "- How frequently we sync model weights from the training model to the target model, which is used to get the value of the next state in the Bellman approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last batch of hyperparameters is related to the epsilon decay schedule. To achieve proper exploration, at early stages of training, we start with epsilon=1.0, which causes all actions to be selected randomly. Then, during first 100,000 frames, epsilon is linearly decayed to 0.02, which corresponds to the random action taken in 2%\n",
    "of steps. A similar scheme was used in the original DeepMind paper, but the duration of decay was 10 times longer (so, epsilon = 0.02 is reached after a million frames).\n",
    "\n",
    "The next chunk of the code defines our experience replay buffer, the purpose of which is to keep the last transitions obtained from the environment (tuples of the observation, action, reward, done flag, and the next state). Each time we do a step in the environment, we push the transition into the buffer, keeping only a fixed number of steps, in our case 10k transitions. For training, we randomly sample the batch of transitions from the replay buffer, which allows us to break the correlation between subsequent steps in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience',\n",
    "field_names=['state', 'action', 'reward', 'done', 'new_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the experience replay buffer code is quite straightforward: it basically exploits the capability of the deque class to maintain the given number of entries in the buffer. In the sample() method, we create a list of random indices and then repack the sampled entries into NumPy arrays for more convenient loss calculation.\n",
    "\n",
    "The next class we need to have is an Agent, which interacts with the environment and saves the result of the interaction into the experience replay buffer that we've just seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the agent's initialization, we need to store references to the environment and experience replay buffer, tracking the current observation and the total reward accumulated so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method of the agent is to perform a step in the environment and store its result in the buffer. To do this, we need to select the action first. With the probability epsilon (passed as an argument) we take the random action, otherwise we use the past model to obtain the Q-values for all possible actions and choose the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        new_state = new_state\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the action has been chosen, we pass it to the environment to get the next observation and reward, store the data in the experience buffer and the handle the end-of-episode situation. The result of the function is the total accumulated reward if we've reached the end of the episode with this step, or None if not.\n",
    "\n",
    "Now it is time for the last function in the training module, which calculates the loss for the sampled batch. This function is written in a form to maximally exploit GPU parallelism by processing all batch samples with vector operations, which makes it harder to understand when compared with a naive loop over the batch. Yet this optimization pays off: the parallel version is more than two times faster than an explicit loop over the batch.\n",
    "\n",
    "As a reminder, here is the loss expression we need to calculate: \n",
    "<img width=\"250px\" src=\"assets/equ7.png\"> \n",
    "for steps which aren't at the end of the episode, or\n",
    "<img width=\"120px\" src=\"assets/equ8.png\"> \n",
    "for final steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "           states, actions, rewards, dones, next_states = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In arguments, we pass our batch as a tuple of arrays (repacked by the sample() method in the experience buffer), our network that we're training and the target network, which is periodically synced with the trained one. The first model (passed as the argument net) is used to calculate gradients, while the second model in the tgt_net argument is used to calculate values for the next states and this calculation shouldn't affect gradients. To achieve this, we're using the detach() function of the PyTorch tensor to prevent gradients from flowing into the target network's graph. This function was described in Chapter 3, Deep Learning with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        states_v = torch.tensor(states).to(device)\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code is simple and straightforward: we wrap individual NumPy arrays with batch data in PyTorch tensors and copy them to GPU if the CUDA device was specified in arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the line above, we pass observations to the first model and extract the specific Q-values for the taken actions using the gather() tensor operation. The first argument to the gather() call is a dimension index that we want to perform gathering on (in our case it is equal to 1, which corresponds to actions). The second argument is a tensor of indices of elements to be chosen. Extra unsqueeze() and squeeze() calls are required to fulfill the requirements of the gather functions to the index argument and to get rid of extra dimensions that we created (the index should have the same number of dimensions as the data we're processing). In the following image, you can see an illustration of what gather does on the example case, with a batch of six entries and four actions.\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/fig3transform.png\">\n",
    "\n",
    "Keep in mind that the result of gather() applied to tensors is a differentiable operation, which will keep all gradients with respect to the final loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        next_state_values = tgt_net(next_states_v).max(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On in the above line, we apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1. Function max() returns both maximum values and indices of those values (so it calculates both max and argmax), which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        next_state_values[done_mask] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make one simple, but very important, point: if transition in the batch is from the last step in the episode, then our value of the action doesn't have a discounted reward of the next state, as there is no next state to gather reward from. This may look minor, but this is very important in practice: without this, training will not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        next_state_values = next_state_value.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this line, we detach the value from its computation graph to prevent gradients from flowing into the neural network used to calculate Q approximation for next states. This is important, as without this our backpropagation of the loss will start to affect both predictions for the current state and the next state. However, we don't want to touch predictions for the next state, as they're used in the Bellman equation to calculate reference Q-values. To block gradients from flowing into this branch of the graph, we're using the detach() method of the tensor, which returns the tensor without connection to its calculation history. In previous versions of PyTorch, we used a volatile attribute of the Variable class, which was obsoleted with the 0.4.0 release. More information was provided in Chapter 3, Deep Learning with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the Bellman approximation value and the mean squared error loss. This ends our loss function calculation, and the rest of the code is our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME, help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "    parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND, help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we create a parser of command-line arguments. Our script allows us to enable CUDA and train on environments that are different from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env = wrappers.make_env(args.env)\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create our environment with all required wrappers applied, the neural network we're going to train, and our target network with the same architecture. In the beginning, they'll be initialized with different random weights, but it doesn't matter much as we'll sync them every 1k frames, which roughly corresponds to one episode of Pong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    writer = SummaryWriter(comment=\"-\" + args.env)\n",
    "    print(net)\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "    epsilon = EPSILON_START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create our experience replay buffer of the required size and pass it to the agent. Epsilon is initially initialized to 1.0, but will be decreased every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last things we do before the training loop are to create an optimizer, a buffer for full episode rewards, a counter of frames and several variables to track our speed, and the best mean reward reached. Every time our mean reward beats the record, we'll save the model in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of the training loop, we count the number of iterations completed and decrease epsilon according to our schedule. Epsilon will drop linearly during the given number of frames (EPSILON_DECAY_LAST_FRAME=100k) and then will be kept on the same level of EPSILON_FINAL=0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (frame_idx, len(total_rewards), mean_reward, epsilon, speed))     \n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code, we ask our agent to make a single step in the environment (using our current network and value for epsilon). This function returns a non- None result only if this step is the final step in the episode. In that case, we report our progress. Specifically, we calculate and show, both in the console and in TensorBoard, these values:\n",
    "- Speed as a count of frames processed per second\n",
    "- Count of episodes played\n",
    "- Mean reward for the last 100 episodes\n",
    "- Current value for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), args.env + \"-best.dat\")\n",
    "            if best_mean_reward is not None:\n",
    "            print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "            if mean_reward > args.reward:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time our mean reward for the last 100 episodes reaches a maximum, we report this and save the model parameters. If our mean reward exceeds the specified boundary, then we stop training. For Pong, the boundary is 19.5, which means winning more than 19 games from 21 possible games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check whether our buffer is large enough for training. In the beginning, we should wait for enough data to start, which in our case is 10k transitions. The next condition syncs parameters from our main network to the target net every SYNC_TARGET_FRAMES, which is 1k by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece of the training loop is very simple, but requires the most time to execute: we zero gradients, sample data batches from the experience replay buffer, calculate loss, and perform the optimization step to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.4. Running and Performance\n",
    "\n",
    "This example is demanding on resources. On Pong, it requires about 400k frames to reach a mean reward of 17 (which means winning more than 80% of games). A similar number of frames will be required to get from 17 to 19.5, as our learning progress saturates and it's hard for the model to improve the score. So, on average, a million frames are needed to train it fully. On the GTX 1080 Ti, I have a speed of about 150 frames per second, which is about two hours of training. On a CPU, the speed is much slower: about nine frames per second, which will take about a day and a half. Remember that this is for Pong, which is relatively easy to solve. Other games require hundreds of millions of frames and a 100 times larger experience replay buffer.\n",
    "\n",
    "In the next chapter, we'll look at various approaches, found by researchers since 2015, which can help to increase both training speed and data efficiency. Nevertheless, for Atari you'll need resources and patience. The following image shows a TensorBoard screenshot with training dynamics:\n",
    "\n",
    "<img width=\"800png\" src=\"assets/characteristic_x.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning of the training:\n",
    "\n",
    "                       rl_book_samples/Chapter06$ ./02_dqn_pong.py --cuda\n",
    "                       DQN (\n",
    "                         (conv): Sequential (\n",
    "                           (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
    "                           (1): ReLU ()\n",
    "                           (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
    "                           (3): ReLU ()\n",
    "                           (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "                           (5): ReLU ()\n",
    "                         )\n",
    "                         (fc): Sequential (\n",
    "                           (0): Linear (3136 -> 512)\n",
    "                           (1): ReLU ()\n",
    "                           (2): Linear (512 -> 6)\n",
    "                    ) )\n",
    "                    \n",
    "    1048: done 1 games, mean reward -19.000, eps 0.99, speed 83.45 f/s\n",
    "    1894: done 2 games, mean reward -20.000, eps 0.98, speed 913.37 f/s\n",
    "    2928: done 3 games, mean reward -20.000, eps 0.97, speed 932.16 f/s\n",
    "    3810: done 4 games, mean reward -20.250, eps 0.96, speed 923.60 f/s\n",
    "    4632: done 5 games, mean reward -20.400, eps 0.95, speed 921.52 f/s\n",
    "    5454: done 6 games, mean reward -20.500, eps 0.95, speed 918.04 f/s\n",
    "    6379: done 7 games, mean reward -20.429, eps 0.94, speed 906.64 f/s\n",
    "    7409: done 8 games, mean reward -20.500, eps 0.93, speed 903.51 f/s\n",
    "    8259: done 9 games, mean reward -20.556, eps 0.92, speed 905.94 f/s\n",
    "    9395: done 10 games, mean reward -20.500, eps 0.91, speed 898.05 f/s\n",
    "    10204: done 11 games, mean reward -20.545, eps 0.90, speed 374.76 f/s\n",
    "    10995: done 12 games, mean reward -20.583, eps 0.89, speed 160.55 f/s\n",
    "    11887: done 13 games, mean reward -20.538, eps 0.88, speed 160.44 f/s\n",
    "    12949: done 14 games, mean reward -20.571, eps 0.87, speed 160.67 f/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hundreds of games later, our DQN should start to figure out how to win one or two games out of 21. The speed has decreased due to epsilon drop: we need to use our model not only for training but also for the environment step.\n",
    "\n",
    "    101032: done 83 games, mean reward -19.506, eps 0.02, speed 143.06 f/s\n",
    "    103349: done 84 games, mean reward -19.488, eps 0.02, speed 142.99 f/s\n",
    "    106444: done 85 games, mean reward -19.424, eps 0.02, speed 143.15 f/s\n",
    "    108359: done 86 games, mean reward -19.395, eps 0.02, speed 143.18 f/s\n",
    "    110499: done 87 games, mean reward -19.379, eps 0.02, speed 143.01 f/s\n",
    "    113011: done 88 games, mean reward -19.352, eps 0.02, speed 142.98 f/s\n",
    "    115404: done 89 games, mean reward -19.326, eps 0.02, speed 143.07 f/s\n",
    "    117821: done 90 games, mean reward -19.300, eps 0.02, speed 143.03 f/s\n",
    "    121060: done 91 games, mean reward -19.220, eps 0.02, speed 143.10 f/s\n",
    "\n",
    "Finally, after many more games, it can finally dominate and beat the (not very sophisticated) built-in Pong AI opponent:\n",
    "\n",
    "    982059: done 520 games, mean reward 19.500, eps 0.02, speed 145.14 f/s\n",
    "    984268: done 521 games, mean reward 19.420, eps 0.02, speed 145.39 f/s\n",
    "    986078: done 522 games, mean reward 19.440, eps 0.02, speed 145.24 f/s\n",
    "    987717: done 523 games, mean reward 19.460, eps 0.02, speed 145.06 f/s\n",
    "    989356: done 524 games, mean reward 19.470, eps 0.02, speed 145.07 f/s\n",
    "    991063: done 525 games, mean reward 19.510, eps 0.02, speed 145.31 f/s\n",
    "    Best mean reward updated 19.500 -> 19.510, model saved\n",
    "    Solved in 991063 frames!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.5. Your Model in Action\n",
    "\n",
    "Just to make your waiting a bit more fun, our code saves the best model's weights. In the Chapter06/03_dqn_play.py file, we have a program which can load this model file and play one episode, displaying the model's dynamics.\n",
    "\n",
    "The code is very simple, but seeing how several matrices, with a million parameters, play Pong with superhuman accuracy, by observing only the pixels, can be like magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from lib import wrappers\n",
    "from lib import dqn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we import the familiar PyTorch and Gym modules. The preceding FPS parameter specifies the approximate speed of the shown frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"—model\", required=True, help=\"Model file to load\")\n",
    "    parser.add_argument(\"-e\", \"—env\", default=DEFAULT_ENV_NAME, help=\"Environment name to use, default=\" + DEFAULT_ENV_NAME)\n",
    "    parser.add_argument(\"-r\", \"—record\", help=\"Directory to store video recording\")\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script accepts the filename of the saved model and allows the specification of the Gym environment (of course, the model and environment have to match). Additionally, you can pass option -r with the name of a non existent directory, which will be used to save a video of your game (using the Monitor wrapper). By default, the script just shows frames, but if you want to upload your model's gameplay to YouTube, for example, -r could be handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env = wrappers.make_env(args.env)\n",
    "    if args.record:\n",
    "        env = gym.wrappers.Monitor(env, args.record)\n",
    "        net = dqn_model.DQN(env.observation_space.shape, env.action_space.n)\n",
    "        net.load_state_dict(torch.load(args.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code should be clear without comments: we create the environment and our model, then we load weights from the file passed in the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    while True:\n",
    "        start_ts = time.time()\n",
    "        env.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost an exact copy of Agent class' method play_step() from the training code, with the lack of epsilon-greedy action selection. We just pass our observation to the agent and select the action with maximum value. The only new thing here is the render() method in the environment, which is a standard way in Gym to display the current observation (you need to have a GUI for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        delta = 1/FPS - (time.time() - start_ts)\n",
    "        if delta > 0:\n",
    "            time.sleep(delta)\n",
    "    print(\"Total reward: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code is also simple. We pass the action to the environment, count the total reward, and stop our loop when the episode ends.\n",
    "\n",
    "Things to try: If you're curious and want to experiment with this chapter's material on your own, then here is a short list of directions to explore. Be warned though: they can take lots of time and can cause you some moments of frustration during your experiments. However, these experiments are a very efficient way to really master the material from a practical point of view:\n",
    "\n",
    "- Try to take some other games from the Atari set, such as Breakout or Atlantis or RiverRaid (my childhood favorite). This could require the tuning of hyperparameters.\n",
    "- As an alternative to FrozenLake, there is another tabular environment, Taxi, which emulates a taxi driver who needs to pick up passengers and take them to a destination.\n",
    "- Play with Pong hyperparameters. Is it possible to train faster? OpenAI claims that it can solve Pong in 30 minutes using the A3C method (which is a subject of part three of this book). Maybe it's possible with a DQN.\n",
    "- Can you make the DQN training code faster? The OpenAI Baselines project has shown 350 frames per second using TensorFlow on GTX 1080 Ti. So, it looks like it's possible to optimize the PyTorch code.\n",
    "- Can you get The Ultimate Pong Dominator model with a mean score of 21? It shouldn't be very hard: learning rate decay is the obvious method to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 05. Summary\n",
    "\n",
    "---\n",
    "\n",
    "In this chapter, we introduced lots of new and complex material. We became familiar with the limitations of value iteration in complex environments with large observation spaces and discussed how to overcome them with Q-learning. We checked the Q-learning algorithm on the FrozenLake environment and discussed the approximation of Q-values with neural networks and the extra complications that arise from this approximation. We covered several tricks for DQNs to improve their training stability and convergence, such as experience replay buffer, target networks, and frame stacking. Finally, we combined those extensions in to one single implementation of DQN that solves the Pong environment from the Atari games suite.\n",
    "\n",
    "In the next chapter, we'll look at a set of tricks that researchers have found, since 2015, to improve DQN convergence and quality, which (combined) can produce state-of-the-art results on most of the 54 Atari games. This set was published in 2017 and we'll analyze and reimplement all of the tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___THE END___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
