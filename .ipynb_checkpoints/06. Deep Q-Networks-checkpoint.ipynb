{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Deep Q-Networks</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this chapter, we'll talk about problems with the Value iteration method and introduce its variation, called __Q-learning__. In particular, we'll look at the application of Q-learning to so-called \"grid world\" environments, which is called __tabular Q-learning__, and then we'll discuss Q-learning in conjunction with neural networks. This combination has the name __DQN__. At the end of the chapter, we'll reimplement a DQN algorithm from the famous paper, *Playing Atari with Deep Reinforcement Learning by V. Mnih and others, published in 2013*, which started a new era in RL development.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=\"700px\" src=\"./assets/dqn.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 01. Real-Life Value Iteration\n",
    "\n",
    "---\n",
    "\n",
    "Let's recall the value iteration method. In every step, loop over all states, and for every state, perform an update of its value with a Bellman approximation. The other variation of this method for Q-values (values for actions) is almost the same. However, in here we approximate and store values for every state and action.\n",
    "\n",
    "Despite the impovement that the value iteration method has over Cross-Entropy method, it has its own limitations. For example:\n",
    "1. __Count of states and ability to loop over them:__ <br> In Value iteration method we assume we know all states in advance and we can iterate over them and store the approximation of the value state. This is only possible with simple environments and not with the complex ones. <br><br>\n",
    "\n",
    "2. __Limited to discrete action sapce only:__ <br> The value iteration approach limits us to discrete action spaces and we cannot use this approch for continuous control problems (where actions can represent continuous variables) such as the angle of a steering wheel, the force on an actuator, or the temperature of a heater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 02. Tabular Q-Learning\n",
    "\n",
    "---\n",
    "\n",
    "There is not need to iterate over every state in the state space. We have an environment that can be used as a source of real-life samples of states. If some state in the state space is not shown to us by the environment, why should we care about its value? We can use states obtained from the environment to update values of states, which can save us lots of work. This modification of the Value iteration method is known as __Q-learning__, and for cases with explicit state-to-value mappings, has the following steps:\n",
    "\n",
    "1. Start with an empty table, mapping states to values of actions.\n",
    "2. By interacting with the environment, obtain the tuple s, a, r, s′ (state, action, reward, and the new state). In this step, we need to decide which action to take with consideration of exploration vs. exploitation.\n",
    "3. Update the Q(s, a) value using the Bellman approximation:\n",
    "<img width=\"300px\" src=\"assets/equ_1.png\">\n",
    "4. Repeat from step 2.\n",
    "\n",
    "As in Value iteration, the end condition could be some threshold of the update or we can perform test episodes to estimate the expected reward from the policy. \n",
    "\n",
    "<br><center>***</center><br>\n",
    "\n",
    "We update the Q-values or Q(s, a) using a __\"blending\"__ technique, which is the average between old and new values of Q using learning rate α (with a value from 0 to 1):\n",
    "\n",
    "<img width=\"480px\" src=\"assets/equ2.png\">\n",
    "\n",
    "This allows values of Q to converge smoothly, even if our environment is noisy.\n",
    "\n",
    "<br><center>***</center><br>\n",
    "So let's take a look at the final version of the algorithm:\n",
    "1. Start with an empty table for Q(s, a).\n",
    "2. Obtain (s, a, r, s′) from the environment.\n",
    "3. Make a Bellman update:\n",
    "<img width=\"480px\" src=\"assets/equ3.png\">\n",
    "4. Check convergence conditions. If not met, repeat from step 2.\n",
    "\n",
    "As mentioned earlier, this method is called __tabular Q-learning__, as we keep a table of states with their Q-values. Now let's try it on our FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2         # Learning rate \n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent class\n",
    "class Agent:\n",
    "    \n",
    "    \n",
    "    # The constructor class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # The environment\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        \n",
    "        # The initial state\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        # Initialize an empty table for Q(s, a)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "        \n",
    "    # Function for obtaining the next transition from the environment\n",
    "    def sample_env(self):\n",
    "\n",
    "        # Take a random action\n",
    "        action = self.env.action_space.sample()\n",
    "\n",
    "        # Update the old state with current state\n",
    "        old_state = self.state\n",
    "\n",
    "        # Take the given action and get the observation (next state), reward, done, info\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "\n",
    "        # Update the current state\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "        # Return S, A, R, S'\n",
    "        return (old_state, action, reward, new_state)\n",
    "\n",
    "    \n",
    "    # Function for finding the best action to take from given state\n",
    "    def best_value_and_action(self, state):\n",
    "\n",
    "        # Initialize the best action value and best action\n",
    "        best_action_value, best_action = None, None\n",
    "\n",
    "        # Iterate through action space\n",
    "        for action in range(self.env.action_space.n):\n",
    "\n",
    "            # Fetch the action value\n",
    "            action_value = self.values[(state, action)]\n",
    "\n",
    "            # If the best action value is none OR it's less than action value\n",
    "            if (best_action_value is None) or (best_action_value < action_value):\n",
    "\n",
    "                # Update the best action value\n",
    "                best_action_value = action_value\n",
    "\n",
    "                # Update the best action\n",
    "                best_action = action\n",
    "\n",
    "        return best_action_value, best_action\n",
    "\n",
    "    \n",
    "    # Function for updating the action value table\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "\n",
    "        # Find the best action value to take\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "\n",
    "        # Calculate the new action value using Bellman approximation \n",
    "        new_val = r + GAMMA * best_v\n",
    "\n",
    "        # Fetch the old action value\n",
    "        old_val = self.values[(s, a)]\n",
    "\n",
    "        # Blend the old and new action value (using learning rate) and update the values table\n",
    "        self.values[(s, a)] = old_val * (1 - ALPHA) + new_val * ALPHA\n",
    "\n",
    "        \n",
    "    # Function for playing one full eopisode\n",
    "    def play_episode(self, env):\n",
    "\n",
    "        # Initialize the total reward\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # Get the initial state\n",
    "        state = env.reset()\n",
    "\n",
    "        # Infinite loop\n",
    "        while True:\n",
    "\n",
    "            # Get the best action\n",
    "            _, action = self.best_value_and_action(state)\n",
    "\n",
    "            # Take the given action and get the observation (next state), reward, done, info\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # Add the reward to total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # If terminal state\n",
    "            if is_done:\n",
    "\n",
    "                # Break the loop\n",
    "                break\n",
    "\n",
    "            # Update the current state to next state\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.100\n",
      "Best reward updated 0.100 -> 0.200\n",
      "Best reward updated 0.200 -> 0.300\n",
      "Best reward updated 0.300 -> 0.400\n",
      "Best reward updated 0.400 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.800\n",
      "Best reward updated 0.800 -> 0.900\n",
      "Solved in 7679 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Execute the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create the environment\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = Agent()\n",
    "    \n",
    "    # Summary writer for tensorboard\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    # Initialize the iteration number\n",
    "    iter_no = 0\n",
    "    \n",
    "    # Initialize the best reward\n",
    "    best_reward = 0.0\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Increment the iteration number\n",
    "        iter_no += 1\n",
    "        \n",
    "        # Obtain the next transition from the environment\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        \n",
    "        # Update the action value table\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "\n",
    "        # Initialize the reward\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Iterate through TEST_EPISODES numbers\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            \n",
    "            # Update the reward by playing one full episode\n",
    "            reward += agent.play_episode(test_env)\n",
    "            \n",
    "        # Divide reward by TEST_EPISODES\n",
    "        reward /= TEST_EPISODES\n",
    "        \n",
    "        # Add the reward value into tensorboard\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        \n",
    "        # If reward is higher than best reward\n",
    "        if reward > best_reward:\n",
    "            \n",
    "            # Print best reward and reward\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            \n",
    "            # Update the best reward to reward\n",
    "            best_reward = reward\n",
    "            \n",
    "        # If reward is higher than 0.8 then it is SOLVED \n",
    "        if reward > 0.80:\n",
    "            \n",
    "            # Print the iteration number which is solved\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            \n",
    "            # break the loop\n",
    "            break\n",
    "            \n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that this version used more iterations to solve the problem compared to the value iteration method from the previous chapter. The reason for that is that we're no longer using the experience obtained during testing. (In Chapter05/02_frozenlake_q_iteration.py, periodical tests cause an update of Q-table statistics. Here we don't touch Q-values during the test, which cause more iterations before the environment gets solved.) Overall, the total amount of samples required from the environment is almost the same. The reward chart in TensorBoard also shows good training dynamics, which are very similar to the value iteration method.\n",
    "\n",
    "<img width=\"350px\" src=\"./assets/dynfrozen.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 03. Deep Q-learning\n",
    "\n",
    "---\n",
    "\n",
    "The Q-Learning method solves the issue with iteration over the states. However, it might struggle if the count of the observable set of states is very large. For example, in Atari games, if we decide to use raw pixels as individual states then there's going to be too many states to track and approximate values for.\n",
    "\n",
    "In some environments, the count of (different) observable states can be almost infinite. For example, in CartPole Environment, there is four floating point numbers represented as the state. The number of combinations of values is finite, but this number is extremely large. We can create some bins to discretize those values. For that we need to decide what ranges of parameters are important to distinguish as different states and what ranges could be clustered together. In the case of Atari games, we can treat two different images with a single pixel change as a single state. However, we need to distinguish some of the states.\n",
    "\n",
    "The following image shows the Pong game. The objective of the game is to get the bounding ball past our opponent's paddle, while preventing it from getting past our paddle (our paddle is green and it's on the right). The situations below are just two from the 10<sup>70802</sup> possible situations, but we want our agent to act on them differently.\n",
    "\n",
    "<img width=\"400px\" src=\"./assets/pongenv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a solution to this problem, we can use a nonlinear representation that maps both state and action onto a value. Using a deep neural network is one of the most popular options, especially when dealing with observations represented as screen images. \n",
    "\n",
    "<br>\n",
    "\n",
    "With this in mind, let's make modifications to the Q-learning algorithm (the following algorithm looks simple but, unfortunately, it won't work very well. In the following sections, we will discuss what can go wrong and show how to resolve it.):\n",
    "1. Initialize Q(s, a) with some initial approximation\n",
    "2. By interacting with the environment, obtain the tuple (s, a, r, s′)\n",
    "3. Calculate loss. \n",
    "    - If episode has ended then:\n",
    "$$ L = (Q_{s,a} − r)^2 $$\n",
    "    <br>\n",
    "    - Or otherwise:\n",
    "\n",
    "<img width=\"300px\" src=\"assets/equ4.png\"> \n",
    "\n",
    "4. Update Q(s, a) using the __stochastic gradient descent (SGD)__ algorithm, by minimizing the loss with respect to the model parameters\n",
    "5. Repeat from step 2 until converged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.1. Interaction With the Environment\n",
    "\n",
    "--- \n",
    "\n",
    "We need to interact with the environment to receive data for training. In simple environments (such as FrozenLake), we can act randomly. However, in complex environments (such as Pong), acting randomly will not work. Alternatively, we use our Q function approximation as a source of behavior (as in the value iteration method, when we remembered our experience during testing).\n",
    "\n",
    "If our representation of Q is good, then the gained experience will be relevant for training. On the other hand, if our approximation is not perfect (usually at the beginning of training), then our agent might stuck with bad actions for some states without ever trying to behave differently. This is the problem know as __exploration vs. exploitation dilemma__. As you can see, exploration (random behavior) is better at the beginning of the training. As our training progresses, we want to do exploitation (fall back to our Q approximation to decide how to act).\n",
    "\n",
    "__Epsilon-Greedy__ is method that performs a mix of exploration and exploitation. In this method, we are switching between random and Q policy using the probability hyperparameter ε. The usual practice is to start with `ε = 1.0` (100% random actions) and slowly decrease it to some small value such as `ε = 0.05` (5% random actions) or `ε = 0.02` (2% random actions). In this way, we explore more in the begining and exploit more in the end. This problem is one of the fundamental open questions in RL and an active area of research, which is not even close to being resolved completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.2. SGD Optimization\n",
    "\n",
    "---\n",
    "\n",
    "In the Q-Learning procedure, we are trying to approximate a complex, nonlinear function Q(s, a) with a neural network. To do this, we calculate targets for this function using the Bellman equation and then pretend that we have a supervised learning problem at hand. \n",
    "\n",
    "One of the requirements for SGD optimization is that the training data is __independent and identically distributed__ (frequently abbreviated as __i.i.d__). In our case, data that we're going to use for the SGD update doesn't fulfill these criteria:\n",
    "\n",
    "1. Our samples are not independent. Even if we accumulate a large batch of data samples, they all will be very close to each other, as they belong to the same episode.\n",
    "\n",
    "2. Distribution of our training data won't be identical to samples provided by the optimal policy that we want to learn. Data that we have is a result of some other policy (our current policy, random, or both in the case of ε-greedy), but we don't want to learn how to play randomly: we want an optimal policy with the best reward.\n",
    "\n",
    "To deal with this problem, we usually need to use a large buffer of our past experience and sample training data from it, instead of using our latest experience. This method is called __replay buffer__. The simplest implementation is a buffer of fixed size, with new data added to the end of the buffer so that it pushes the oldest experience out of it. Replay buffer allows us to train on more-or-less independent data, but data will still be fresh enough to train on samples generated by our recent policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.3. Correlation Between Steps\n",
    "\n",
    "---\n",
    "\n",
    "Another practical issue with the default training procedure is also related to the lack of i.i.d in our data, but in a slightly different manner. The Bellman equation provides us with the value of Q(s, a) via Q(s′, a′) (which has the name of __bootstrapping__). However, both states s and s′ have only one step between them. This makes them very similar and it's really hard for neural networks to distinguish between them. When we perform an update of our network's parameters, to make Q(s, a) closer to the desired result, we indirectly can alter the value produced for Q(s′, a′) and other states nearby. This can make our training really unstable, like chasing our own tail: when we update Q for state s, then on subsequent states we discover that Q(s′, a′) becomes worse, but attempts to update it can spoil our Q(s, a) approximation, and so on.\n",
    "\n",
    "To make training more stable, there is a trick, called __target network__, when we keep a copy of our network and use it for the Q(s′, a′) value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in N steps (where N is usually quite a large hyperparameter, such as 1k or 10k training iterations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.4. The Markov Property\n",
    "\n",
    "---\n",
    "\n",
    "Our RL methods use MDP formalism as their basis, which assumes that the environment obeys the Markov property: observation from the environment is all that we need to act optimally (in other words, our observations allow us\n",
    "to distinguish states from one another). As we've seen on the preceding Pong's screenshot, one single image from the Atari game is not enough to capture all important information (using only one image we have no idea about the speed and direction of objects, like the ball and our opponent's paddle). This obviously violates the Markov property and moves our single-frame Pong environment into the area of __partially observable MDPs (POMDP)__. A POMDP is basically MDP without the Markov property and they are very important in practice. For example, for most card games where you don't see your opponents' cards, game observations are POMDPs, because current observation (your cards and cards on the table) could correspond to different cards in your opponents' hands.\n",
    "\n",
    "We'll not discuss POMPDs in detail in this book, so, for now, we'll use a small technique to push our environment back into the MDP domain. The solution is maintaining several observations from the past and using them as a state. In the case of Atari games, we usually stack k subsequent frames together and use them as the observation at every state. This allows our agent to deduct the dynamics of the current state, for instance, to get the speed of the ball and its direction. The usual \"classical\" number of k for Atari is four OR `k=4`. Of course, it's just a hack, as there can be longer dependencies in the environment, but for most of the games it works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 03.5. The Final Form of DQN Training\n",
    "\n",
    "---\n",
    "\n",
    "ε-greedy, replay buffer, and target network are the basis that allows DeepMind to successfully train a DQN on a set of 49 Atari games and demonstrate the efficiency of this approach applied to complicated environments.\n",
    "\n",
    "The original paper (without target network) was published at the end of 2013 (___Playing Atari with Deep Reinforcement Learning, Mnih and others.___), and they used seven games for testing. Later, at the beginning of 2015, a revised version of the article, with 49 different games, was published in Nature (___Human-Level Control Through Deep Reinforcement Learning, Mnih and others.___)\n",
    "\n",
    "<br>\n",
    "\n",
    "The algorithm for DQN from the preceding papers has the following steps:\n",
    "1. Initialize:\n",
    "    - Parameters for <code>Q(s, a)</code> and <code>Q&#x0302;(s, a)</code> with random weights\n",
    "    - `ε←1.0`\n",
    "    - Empty replay buffer\n",
    "    \n",
    "    2. With probability `ε`, select a random action `a`, otherwise <code>a = argmax<sub>a</sub>Q<sub>s,a</sub></code>\n",
    "\n",
    "3. Execute action a in an emulator and observe reward `r` and the next state `s′`\n",
    "\n",
    "4. Store transition `(s, a, r, s′)` in the replay buffer\n",
    "\n",
    "5. Sample a random minibatch of transitions from the replay buffer\n",
    "\n",
    "6. For every transition in the buffer, calculate target `y=r` if the episode has ended at this step or otherwise:\n",
    "<img width=\"250px\" src=\"assets/equ5.png\"> \n",
    "\n",
    "7. Calculate loss:\n",
    "<img width=\"145px\" src=\"assets/equ6.png\"> \n",
    "\n",
    "8. Update `Q(s, a)` using the SGD algorithm by minimizing the loss in respect to model parameters\n",
    "\n",
    "9. Every N steps copy weights from `Q` to <code>Q&#x0302;</code>\n",
    "\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's implement it now and try to beat some of the Atari games!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 04. DQN on Pong\n",
    "\n",
    "---\n",
    "\n",
    "This example has been split into three modules due to its length, logical structure, and reusability. The modules are as follows:\n",
    "\n",
    "- __Chapter06/lib/wrappers.py:__ These are Atari environment wrappers mostly taken from the OpenAI Baselines project\n",
    "- __Chapter06/lib/dqn_model.py:__ This is the DQN neural net layer, with the same architecture as the DeepMind DQN from the Nature paper\n",
    "- __Chapter06/02_dqn_pong.py:__ This is the main module with the training loop, loss function calculation, and experience replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.1. Wrappers\n",
    "\n",
    "---\n",
    "\n",
    "Tackling Atari games with RL is quite demanding from a resource perspective. To make things faster, several transformations are applied to the Atari platform interaction, which are described in DeepMind's paper. Transformations are usually implemented as OpenAI Gym wrappers of various kinds. \n",
    "\n",
    "The full list is quite lengthy and there are several implementations of the same wrappers in various sources. My personal favorite is in the OpenAI repository called __baselines__, which is a set of RL methods and algorithms implemented in TensorFlow and applied to popular benchmarks, to establish the common ground for comparing methods. The repository is available from https://github.com/openai/baselines, and wrappers are available in this file: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py.\n",
    "\n",
    "The full list of Atari transformations used by RL researchers includes:\n",
    "\n",
    "1.  __Convert each lives in the game into seperate episode:__ <br>In general, an episode contains all the steps from the beginning of the game until the \"Game over\" screen, which can last for thousands of game steps. Also, in arcade games, the player is given several lives. Our transformation splits a full episode into individual small episodes. This usually helps to speed up convergence since our episodes become shorter.\n",
    "\n",
    "\n",
    "2. __Perform random (up to 30) no-op actions in the beginning:__ <br> This stabilizes the training. However, there is no proper explanation why it is the case.\n",
    "\n",
    "\n",
    "3. __Make action decision every k steps (Usually `k=3` or `k=4`):__ <br> Since on intermediate frames, the chosen action is simply repeated. Then, this will speed up the training process.\n",
    "\n",
    "\n",
    "4. __Take the maximum of every pixel in the last two frames and use it as an observation:__ <br> Some Atari games have a flickering effect, which is due to the platform's limitation (Atari has a limited amount of sprites that can be shown on a single frame). For a human eye, such quick changes are not visible, but they can confuse neural networks.\n",
    "\n",
    "\n",
    "5. __Pressing FIRE in the beginning of the game:__ <br> Some games (including Pong and Breakout) require a user to press the FIRE button to start the game. In theory, it's possible for a neural network to learn to press FIRE itself, but it will require much more episodes to be played. So, we press FIRE in the wrapper.\n",
    "\n",
    "\n",
    "6. __Scale each frame down from `210×160` (three color frames), into `84×84` (single-color):__ <br> Different approaches are possible. For example, the DeepMind paper describes this transformation as taking the Y-color channel from the YCbCr color space and then rescaling the full image to an 84 × 84 resolution. Some other researchers do grayscale transformation, cropping non-relevant parts of the image and then scaling down. In the Baselines repository (and in the following example code), the latter approach is used.\n",
    "\n",
    "\n",
    "7. __Stack subsequent frames (usually four) together:__ <br> This gives the network the information about the dynamics of the game's objects.\n",
    "\n",
    "\n",
    "8. __Clip rewards to -1, 0, 1:__ <br> Among different games, scores can vary wildly. This spread in reward values makes our loss have completely different scales between the games, which makes it harder to find common hyperparameters for a set of games. To fix this, reward just gets clipped.\n",
    "\n",
    "\n",
    "9. __Convert observations from unsigned bytes to float32 values and rescale:__ <br> The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for a neural network. So, we need to convert the image into floats and rescale the values to the range from 0.0 to 1.0.\n",
    "\n",
    "<br>\n",
    "\n",
    "In the Pong example, we don't need some of the above wrappers, such as converting lives into separate episodes and reward clipping, so those wrappers aren't included in the example code. However, you should be aware of them, just\n",
    "in case you decide to experiment with other games. \n",
    "\n",
    "Sometimes, when the DQN is not converging, the problem is not in the code but in the wrongly wrapped environment. I've spend several days debugging convergence issues caused by missing the __FIRE__ button press at the beginning of a game!\n",
    "\n",
    "Let's take a look at the implementation of individual wrappers from Chapter06/lib/wrappers.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import cv2\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *For knowing purpose only*\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    This wrapper presses the FIRE button in the beginning of environments since it's required to start the game. \n",
    "    In addition, this wrapper checks for several corner cases that are present in some games.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, env = None):\n",
    "        \n",
    "        # Call the parent's constructor to initialize themselves\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        \n",
    "        # Make sure the second item in the action space is 'FIRE'\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        \n",
    "        # Make sure that the action space is greater or euqal to three\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "        \n",
    "    # Step function\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Take the action and get the observation, reward, is_done, info\n",
    "        return self.env.step(action)\n",
    "\n",
    "    \n",
    "    # Reset function\n",
    "    def reset(self):\n",
    "        \n",
    "        # Reset the environment and get the initial observation\n",
    "        self.env.reset()\n",
    "        \n",
    "        # Take action '1' and get the observation, reward, is_done, info\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        \n",
    "        # If terminal state then reset the environment\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        \n",
    "        # Take action '2' and get the observation, reward, is_done, info\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        \n",
    "        # If terminal state then reset the environment\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "            \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    This wrapper combines the repetition of actions during K frames and pixels from two consecutive frames.\n",
    "    Return only every `skip`-th frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, env = None, skip = 4):\n",
    "        \n",
    "        # Call the parent's constructor to initialize themselves\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        \n",
    "        # Most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen = 2)\n",
    "        \n",
    "        # The skipping number (usually 3 or 4)\n",
    "        self._skip = skip\n",
    "\n",
    "        \n",
    "    # Step function\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Initialize the total reward\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # Initialize 'done' with None\n",
    "        done = None\n",
    "        \n",
    "        # Iterate through number of 'skip's\n",
    "        for _ in range(self._skip):\n",
    "            \n",
    "            # Take the action and get the observation, reward, is_done, info\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # Append the observation into _obs_buffer\n",
    "            self._obs_buffer.append(obs)\n",
    "            \n",
    "            # Add reward into total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # If terminal state\n",
    "            if done:\n",
    "                \n",
    "                # Break the loop\n",
    "                break\n",
    "                \n",
    "        # Get the maximum of every pixel in the last two frames (since 'maxlen=2') and use it as an observation         \n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        \n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    \n",
    "    # Reset function\n",
    "    def reset(self):\n",
    "        \n",
    "        # Clear past frame buffer\n",
    "        self._obs_buffer.clear()\n",
    "        \n",
    "        # Reset the environment and get the initial observation\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        # Append the initial observation into _obs_buffer\n",
    "        self._obs_buffer.append(obs)\n",
    "        \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    This wrapper combines the repetition of actions during K frames and pixels from two consecutive frames.\n",
    "    The goal of this wrapper is to convert input observations from the emulator, which normally has a resolution \n",
    "    of 210 × 160 pixels with RGB color channels, to a grayscale 84 × 84 image. It does this using a colorimetric \n",
    "    grayscale conversion (which is closer to human color perception than a simple averaging of color channels), \n",
    "    resizing the image and cropping the top and bottom parts of the result.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, env = None):\n",
    "        \n",
    "        # Call parent's constructor to initialize themselves\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        \n",
    "        # Initialize the frame size\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    \n",
    "    # Function for calling process method (for processing the observation)\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    \n",
    "    # @staticmethod is A static method does not receive an implicit first argument.\n",
    "    # Process function\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        \n",
    "        # If size is 100'800 then convert it to 210x160x3 + Convert to float data tyep\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "            \n",
    "        # If size is 120'000 then convert to 250x160x3 + Convert to float data tyep\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "            \n",
    "        # If other size then throgh an error    \n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "            \n",
    "        # Multiply each image color to some specific number\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        \n",
    "        # Resize image into 84x110\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Crop the screen\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        \n",
    "        # Reshape the image into 84x84x1\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        \n",
    "        # Convert to unit8 data type and return it \n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    This simple wrapper changes the shape of the observation from HWC to the CHW format required by PyTorch. The \n",
    "    input shape of the tensor has a color channel as the last dimension, but PyTorch's convolution layers assume \n",
    "    the color channel to be the first dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        # Call constructor's parent to initialize themselves\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        \n",
    "        # Get the old observation space\n",
    "        old_shape = self.observation_space.shape\n",
    "        \n",
    "        # Initialize the observation space\n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, \n",
    "                                                high = 1.0,\n",
    "                                                shape = (old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype = np.float32)\n",
    "\n",
    "    # Observation function\n",
    "    def observation(self, observation):\n",
    "        \n",
    "        # Move axis of observation and return it\n",
    "        return np.moveaxis(a = observation, source = 2, destination = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    This wrapper converts observation data from bytes to floats and scales every pixel's value to the \n",
    "    range from 0.0 to 1.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Observation function\n",
    "    def observation(self, obs):\n",
    "        \n",
    "        # Convert to floats + scale pixel values to [0, 1]\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    This class creates a stack of subsequent frames along the first dimension and returns them as an observation. \n",
    "    The purpose is to give the network an idea about the dynamics of the objects, such as the speed and direction \n",
    "    of the ball in Pong or how enemies are moving. This is very important information, which it is not possible \n",
    "    to obtain from a single image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, env, n_steps, dtype = np.float32):\n",
    "        \n",
    "        # Call parent's constructor to initiallize themselves\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        \n",
    "        # Initialize the data type\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Initialize the old observation space\n",
    "        old_space = env.observation_space\n",
    "        \n",
    "        # Initialize the observation space\n",
    "        self.observation_space = gym.spaces.Box(low = old_space.low.repeat(n_steps, axis=0),\n",
    "                                                high = old_space.high.repeat(n_steps, axis=0), \n",
    "                                                dtype = dtype)\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self):\n",
    "        \n",
    "        # Initialize the buffer\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype = self.dtype)\n",
    "        \n",
    "        # Put the initial observation into observation function and return it\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    \n",
    "    # Observation function\n",
    "    def observation(self, observation):\n",
    "        \n",
    "        #\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        \n",
    "        #\n",
    "        self.buffer[-1] = observation\n",
    "        \n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating an environment by its name and applying all the required wrappers to it\n",
    "def make_env(env_name):\n",
    "    \n",
    "    # Create an environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Combine the repetition of actions during K frames (k=4) and pixels from two consecutive frames\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    \n",
    "    # Press the FIRE button in the beginning of environments\n",
    "    env = FireResetEnv(env)\n",
    "    \n",
    "    # Process the frames\n",
    "    env = ProcessFrame84(env)\n",
    "    \n",
    "    # Change the shape of the observation from HWC to the CHW format\n",
    "    env = ImageToPyTorch(env)\n",
    "    \n",
    "    # Create a stack of subsequent frames along the first dimension and return them as an observation\n",
    "    env = BufferWrapper(env, 4)\n",
    "    \n",
    "    # Convert to floats + scale pixel values to [0, 1] and then return it\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.2. DQN Model\n",
    "\n",
    "---\n",
    "\n",
    "The model published in Nature has three convolution layers followed by two fully connected layers. All layers are separated by ReLU nonlinearities. The output of the model is Q-values for every action available in the environment, without nonlinearity applied (as Q-values can have any value). The approach to have all Q-values calculated with one pass through the network helps us to increase speed significantly in comparison to treating Q(s, a) literally and feeding observations and actions to the network to obtain the value of the action.\n",
    "\n",
    "The code of the model is in Chapter06/lib/dqn_model.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network Class\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        \n",
    "        # Call parent's constructor to initialize themselves\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Network\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels = input_shape[0], out_channels = 32, kernel_size = 8, stride = 4),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
    "                                  nn.ReLU())\n",
    "\n",
    "        # Get the size of output\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features = conv_out_size, out_features = 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(in_features = 512, out_features = n_actions))\n",
    "\n",
    "    \n",
    "    # Function for getting number of parameters from given shape\n",
    "    def _get_conv_out(self, shape):\n",
    "        \"\"\"\n",
    "        This function accepts the input shape and applies the convolution layer to a fake tensor of such a shape. \n",
    "        The result of the function will be equal to the number of parameters returned by this application. For\n",
    "        example, for 84 × 84 input, the output from the convolution layer will have 3136 values\n",
    "        \"\"\"\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    \n",
    "    # Function for forward pass the network\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This function accepts a 4D input tensor (batch size, color channel, third and fourth are image dimensions). \n",
    "        The application of transformations is done in two steps:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the input into conv layers\n",
    "        conv_out = self.conv(x)\n",
    "        \n",
    "        # Flatten the the 3D tensor into into 2 dimensional array of (batch size, parameters)   # PyTorch doesn't have a 'flatter' layer\n",
    "        conv_out = conv_out.view(x.size()[0], -1)\n",
    "        \n",
    "        # Pass the output into fcl to get the Q-Values and then return it\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.3. Training\n",
    "\n",
    "---\n",
    "\n",
    "The third module contains the experience replay buffer, the agent, the loss function calculation, and the training loop itself. Before going into the code, something needs to be said about the training hyperparameters. DeepMind's Nature paper contained a table with all the details about hyperparameters used to train its model on all\n",
    "49 Atari games used for evaluation. DeepMind kept all those parameters the same for all games (but trained individual models for every game), and it was the team's intention to show that the method is robust enough to solve lots of games with varying complexity, action space, reward structure, and other details using one single model architecture and hyperparameters. However, our goal here is much more modest: we want to solve just the Pong game.\n",
    "\n",
    "Pong is quite simple and straightforward in comparison to other games in the Atari test set, so the hyperparameters in the paper are overkill for our task. For example, to get the best result on all 49 games, DeepMind used a million-observations replay buffer, which requires approximately 20 GB of RAM to keep and lots of samples from the environment to populate. The epsilon decay schedule that was used is also not the best for a single Pong game. In the training, DeepMind linearly decayed epsilon from 1.0 to 0.1 during the first million frames obtained from the environment. However, my own experiments have shown that for Pong, it's enough to decay epsilon over the first 100k frames and then keep it stable. The replay buffer can also be much smaller: 10k transitions will be enough. In the following example, I've used my parameters. These differ from the parameters in the paper but allow us to solve Pong about ten times faster. On a GeForce GTX 1080 Ti, the following version converges to a mean score of 19.5 in one to two hours, but with DeepMind's hyperparameters it will require at least a day.\n",
    "\n",
    "This speed up, of course, is fine-tuning for one particular environment and can break convergence on other games. You're free to play with the options and other games from the Atari set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from dqn_lib import wrappers            # Local import from dgn_lib repository and from wrappers.py file\n",
    "from dqn_lib import dqn_model           # Local import from dgn_lib repository and from dqn_model.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"  # Environment name\n",
    "DEVICE_TYPE = \"cpu\"                      # CPU or GPU\n",
    "MEAN_REWARD_BOUND = 19.5                 # Reward boundary for the last 100 episodes to stop training\n",
    "\n",
    "GAMMA = 0.99                 # Gamma value in Bellman approximation\n",
    "BATCH_SIZE = 32              # Batch size sampled from the replay buffer\n",
    "REPLAY_SIZE = 10000          # Maximum capacity of the buffer\n",
    "REPLAY_START_SIZE = 10000    # Count of frames that we wait before starting training to populate the replay buffer \n",
    "LEARNING_RATE = 1e-4         # Learning rate used in the Adam optimizer\n",
    "SYNC_TARGET_FRAMES = 1000    # Frequently we sync model weights from the training model to the target model, which is used to get the value of the next state in the Bellman approximation\n",
    "\n",
    "EPSILON_START = 1.0                # Start epsilon\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5   # Total of 100'000 frames\n",
    "EPSILON_FINAL = 0.02               # End epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer for keeping the transitions obtained from environment\n",
    "Experience = collections.namedtuple('Experience', field_names = ['state', 'action', 'reward', 'done', 'new_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "class ExperienceBuffer:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        # Initialize the buffer to have 'capacity' entries only (limited amount of entries)\n",
    "        self.buffer = collections.deque(maxlen = capacity)\n",
    "       \n",
    "    \n",
    "    # Method for returning the length of buffer\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    # Method for appending experience into buffer\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    \n",
    "    # Method Sampling from the experience buffer\n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        # Create a list of random indices\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace = False)\n",
    "        \n",
    "        # Get the indices from buffer and repack it\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        # Get the output in the right format\n",
    "        output = np.array(states), \\\n",
    "                 np.array(actions), \\\n",
    "                 np.array(rewards, dtype = np.float32), \\\n",
    "                 np.array(dones, dtype = np.uint8), \\\n",
    "                 np.array(next_states)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent\n",
    "class Agent:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, env, exp_buffer):    \n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.env = env\n",
    "        \n",
    "        # Initialize the experience replay\n",
    "        self.exp_buffer = exp_buffer\n",
    "        \n",
    "        # Initialize the reset method\n",
    "        self._reset()\n",
    "\n",
    "        \n",
    "    # Method for reseting\n",
    "    def _reset(self):\n",
    "        \n",
    "        # Reset the environment and get the initial observation\n",
    "        self.state = env.reset()\n",
    "        \n",
    "        # Initialize the total reward with 0\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        \n",
    "    # Function for perfoming a step and store the result in buffer\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        \n",
    "        # Initialize the end total reward with None\n",
    "        done_reward = None\n",
    "        \n",
    "        # If a random probability is LESS than epsilon\n",
    "        if np.random.random() < epsilon:\n",
    "            \n",
    "            # Take a random action in action space\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # If a random probability is MORE than epsilon\n",
    "        else:\n",
    "            \n",
    "            # Convert state into array\n",
    "            state_a = np.array([self.state], copy = False)\n",
    "            \n",
    "            # Convert state array into tensor\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            \n",
    "            # Pass forward the state into neural network\n",
    "            q_vals_v = net(state_v)\n",
    "            \n",
    "            # Get the maximum value (which is the action tensor to take)\n",
    "            _, act_v = torch.max(q_vals_v, dim = 1)\n",
    "            \n",
    "            # Get the action in the right data type\n",
    "            action = int(act_v.item())\n",
    "        \n",
    "        # Take action and get the new observation, reward, is_done, info\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        \n",
    "        # Add reward into the total reward\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # Add the (S, A, R, is_done, S') into the experience namedtuple\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        \n",
    "        # Append the experience namedtuple into experience buffer\n",
    "        self.exp_buffer.append(exp)\n",
    "        \n",
    "        # Update the current state into new state\n",
    "        self.state = new_state\n",
    "        \n",
    "        # If terminal state\n",
    "        if is_done:\n",
    "            \n",
    "            # Assign the end total reward into done_reward\n",
    "            done_reward = self.total_reward\n",
    "            \n",
    "            # Reset the environment + set the total reward into 0\n",
    "            self._reset()\n",
    "            \n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculate the lost\n",
    "def calc_loss(batch, net, tgt_net, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculate the loss for the sampled batch. \n",
    "    Note: Revisit the loss expression that has been written before.\n",
    "    \n",
    "    PARAMETERS\n",
    "    =========================\n",
    "        - batch: The batch to calculate the loss\n",
    "        - net: The network which is used to calculate gradients.\n",
    "        - tgt_net: Target network which periodically synced with the trained one. is used to calculate values for \n",
    "                   the next states and this calculation shouldn't affect gradients.\n",
    "        - device: CPU or GPU\n",
    "    \"\"\"\n",
    "    \n",
    "    # Repack the batch\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    # Convert states into tensors\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    \n",
    "    # Convert the next states into tensors\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    \n",
    "    # Convert actions into tensors\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    \n",
    "    # Convert rewards into tensors\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    \n",
    "    # Convert dones into bytes\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    First, pass the observation to the first model. Then extract the Q-values for taken actions\n",
    "    - Argument 1: Dimension index to perform gathering. Here, we use 1 since it's actions\n",
    "    - Argument 2: A tensor of element indices to be chosen.\n",
    "\n",
    "    Note 1: unsqueeze() and squeeze() are used to fulfill the requirements of the gather functions and to get rid of \n",
    "    extra dimensions that we created (index should have the same dimensions as the data we're processing). \n",
    "\n",
    "    Note 2: See below for checking out the exact illustration.\n",
    "    \"\"\"\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Pass forward the next state in the target network + Calculate the maximum Q-value along action dimension 1\n",
    "    # + Get the first value (since max() returns max and argmax)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    \n",
    "    # If transition in the batch is from the last step in the episode, then our value of the action doesn't have a \n",
    "    # discounted reward of the next state, as there is no next state to gather reward from\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    \n",
    "    \"\"\"\n",
    "    Detach values in order to prevent gradients from flowing into the neural network (for Q approximation calculation\n",
    "    in next states).\n",
    "\n",
    "    Without this our backpropagation of the loss will affect the predictions for the current state and the next state. \n",
    "    However, we don't want to touch predictions for the next state, as they're used in the Bellman equation to calculate \n",
    "    reference Q-values. \n",
    "\n",
    "    To block gradients from flowing into this branch of the graph, we're using the detach() method of the tensor, which \n",
    "    returns the tensor without connection to its calculation history. \n",
    "    \"\"\"\n",
    "    next_state_values = next_state_value.detach()\n",
    "\n",
    "    # Calculate the Bellman approximation value\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "\n",
    "    # Calculate the mean squared error loss\n",
    "    loss = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following image, you can see an illustration of what gather does on the example case, with a batch of six entries and four actions.\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/fig3transform.png\">\n",
    "\n",
    "Keep in mind that the result of gather() applied to tensors is a differentiable operation, which will keep all gradients with respect to the final loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the device type (CPU or GPU)\n",
    "    device = torch.device(DEVICE_TYPE)\n",
    "    \n",
    "    # Create the environment\n",
    "    env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
    "    \n",
    "    # Initialize the network\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    \n",
    "    # Initialize the target network\n",
    "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    \n",
    "    # Initialize the writer for TensorBoard\n",
    "    writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    "\n",
    "    # Initialize the experience replay buffer of the required size\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    \n",
    "    # Initialize the agent and pass the env and buffer\n",
    "    agent = Agent(env, buffer)\n",
    "    \n",
    "    # Set the epsilon\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    # Initialize the Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "    \n",
    "    # Initialize the total reward\n",
    "    total_rewards = []\n",
    "    \n",
    "    # Initialize the frame index counter\n",
    "    frame_idx = 0\n",
    "    \n",
    "    # Initialize ts_frame to track our speed\n",
    "    ts_frame = 0\n",
    "    \n",
    "    # Start the time - t(s) or time(second)\n",
    "    ts = time.time()\n",
    "    \n",
    "    # Initialize the best mean reward so whenever the mean reward beats the record, we'll save the model\n",
    "    best_mean_reward = None\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Increment the frame index\n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Get the epsilon which it decreases linearly during the given number of frames (EPSILON_DECAY_LAST_FRAME=100k) \n",
    "        # and then will be kept on the same level of EPSILON_FINAL=0.02\n",
    "        epsilon = max(EPSILON_FINAL, \n",
    "                      EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "        \n",
    "        # Perfome one step and get the final reward\n",
    "        reward = agent.play_step(net, epsilon, device = device)\n",
    "        \n",
    "        # If there is reward (since this function returns a non-None result if this is the final step in the episode)\n",
    "        if reward is not None:\n",
    "            \n",
    "            # Append the reward into total_rewards\n",
    "            total_rewards.append(reward)\n",
    "            \n",
    "            # Calculate the speed (as the count of frames processed per second)\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            \n",
    "            # Update the ts_frame\n",
    "            ts_frame = frame_idx\n",
    "            \n",
    "            # Update the current time\n",
    "            ts = time.time()\n",
    "            \n",
    "            # Get the mean of rewards for the last 100 episodes\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            \n",
    "            # Report the progress\n",
    "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (frame_idx, \n",
    "                                                                                     len(total_rewards), \n",
    "                                                                                     mean_reward, \n",
    "                                                                                     epsilon, \n",
    "                                                                                     speed))\n",
    "            \n",
    "            # Add values to TensorBoard\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "            \n",
    "            # If the best mean reward is none OR it's less than mean_reward\n",
    "            if (best_mean_reward is None) or (best_mean_reward < mean_reward):\n",
    "                \n",
    "                # Save the network\n",
    "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "                \n",
    "                # If the best mean reward is non-None\n",
    "                if best_mean_reward is not None:\n",
    "                    \n",
    "                    # Report the update for best mean reward \n",
    "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "                    \n",
    "                # Update the best mean reward ot mean_reward\n",
    "                best_mean_reward = mean_reward\n",
    "                \n",
    "            # If mean reward exceeds the boundary (For Pong, the boundary is 19.5, which means winning more than 19 games from 21 possible games.)\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                \n",
    "                # Print solved\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                \n",
    "                # break the loop\n",
    "                break\n",
    "                \n",
    "\n",
    "        # Check whether our buffer is large enough for training (in our case, it's 10k transitions)\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            \n",
    "            # Go to the beginning of loop\n",
    "            continue\n",
    "            \n",
    "\n",
    "        # Every SYNC_TARGET_FRAMES (which is 1k by default)\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            \n",
    "            # Sync parameters from  main network to the target net\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "            \n",
    "\n",
    "        # Zero out the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Sample data batches from the experience replay buffer\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss_t = calc_loss(batch, net, tgt_net, device = device)\n",
    "        \n",
    "        # Do the backward propagation\n",
    "        loss_t.backward()\n",
    "        \n",
    "        # Perform the optimization step to minimize the loss\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.4. Running and Performance\n",
    "\n",
    "---\n",
    "\n",
    "This example is demanding on resources. On Pong, it requires about 400k frames to reach a mean reward of 17 (which means winning more than 80% of games). A similar number of frames will be required to get from 17 to 19.5, as our learning progress saturates and it's hard for the model to improve the score. \n",
    "\n",
    "So, on average, a million frames are needed to train it fully. On the GTX 1080 Ti, I have a speed of about 150 frames per second, which is about two hours of training. On a CPU, the speed is much slower: about nine frames per second, which will take about a day and a half. Remember that this is for Pong, which is relatively easy to solve. Other games require hundreds of millions of frames and a 100 times larger experience replay buffer.\n",
    "\n",
    "In the next chapter, we'll look at various approaches, found by researchers since 2015, which can help to increase both training speed and data efficiency. Nevertheless, for Atari you'll need resources and patience. The following image shows a TensorBoard screenshot with training dynamics:\n",
    "\n",
    "<img width=\"800png\" src=\"assets/characteristic_x.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In the beginning of the training:\n",
    "                    \n",
    "    1048: done 1 games, mean reward -19.000, eps 0.99, speed 83.45 f/s\n",
    "    1894: done 2 games, mean reward -20.000, eps 0.98, speed 913.37 f/s\n",
    "    2928: done 3 games, mean reward -20.000, eps 0.97, speed 932.16 f/s\n",
    "    3810: done 4 games, mean reward -20.250, eps 0.96, speed 923.60 f/s\n",
    "    4632: done 5 games, mean reward -20.400, eps 0.95, speed 921.52 f/s\n",
    "    5454: done 6 games, mean reward -20.500, eps 0.95, speed 918.04 f/s\n",
    "    6379: done 7 games, mean reward -20.429, eps 0.94, speed 906.64 f/s\n",
    "    7409: done 8 games, mean reward -20.500, eps 0.93, speed 903.51 f/s\n",
    "    8259: done 9 games, mean reward -20.556, eps 0.92, speed 905.94 f/s\n",
    "    9395: done 10 games, mean reward -20.500, eps 0.91, speed 898.05 f/s\n",
    "    10204: done 11 games, mean reward -20.545, eps 0.90, speed 374.76 f/s\n",
    "    10995: done 12 games, mean reward -20.583, eps 0.89, speed 160.55 f/s\n",
    "    11887: done 13 games, mean reward -20.538, eps 0.88, speed 160.44 f/s\n",
    "    12949: done 14 games, mean reward -20.571, eps 0.87, speed 160.67 f/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Hundreds of games later, our DQN should start to figure out how to win one or two games out of 21. The speed has decreased due to epsilon drop: we need to use our model not only for training but also for the environment step.\n",
    "\n",
    "    101032: done 83 games, mean reward -19.506, eps 0.02, speed 143.06 f/s\n",
    "    103349: done 84 games, mean reward -19.488, eps 0.02, speed 142.99 f/s\n",
    "    106444: done 85 games, mean reward -19.424, eps 0.02, speed 143.15 f/s\n",
    "    108359: done 86 games, mean reward -19.395, eps 0.02, speed 143.18 f/s\n",
    "    110499: done 87 games, mean reward -19.379, eps 0.02, speed 143.01 f/s\n",
    "    113011: done 88 games, mean reward -19.352, eps 0.02, speed 142.98 f/s\n",
    "    115404: done 89 games, mean reward -19.326, eps 0.02, speed 143.07 f/s\n",
    "    117821: done 90 games, mean reward -19.300, eps 0.02, speed 143.03 f/s\n",
    "    121060: done 91 games, mean reward -19.220, eps 0.02, speed 143.10 f/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally, after many more games, it can finally dominate and beat the (not very sophisticated) built-in Pong AI opponent:\n",
    "\n",
    "    982059: done 520 games, mean reward 19.500, eps 0.02, speed 145.14 f/s\n",
    "    984268: done 521 games, mean reward 19.420, eps 0.02, speed 145.39 f/s\n",
    "    986078: done 522 games, mean reward 19.440, eps 0.02, speed 145.24 f/s\n",
    "    987717: done 523 games, mean reward 19.460, eps 0.02, speed 145.06 f/s\n",
    "    989356: done 524 games, mean reward 19.470, eps 0.02, speed 145.07 f/s\n",
    "    991063: done 525 games, mean reward 19.510, eps 0.02, speed 145.31 f/s\n",
    "    Best mean reward updated 19.500 -> 19.510, model saved\n",
    "    Solved in 991063 frames!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 04.5. Your Model in Action\n",
    "\n",
    "---\n",
    "\n",
    "Just to make your waiting a bit more fun, our code saves the best model's weights. In the Chapter06/03_dqn_play.py file, we have a program which can load this model file and play one episode, displaying the model's dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from dqn_lib import wrappers\n",
    "from dqn_lib import dqn_model\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25                # FPS (frame-per-second) parameter specifies the approximate speed of the shown frames.\n",
    "record = \"./record\"     # Directory to store video recording\n",
    "visualize = True        # Disable visualization of the game play\n",
    "model = None            # Model file to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create the environment\n",
    "    env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
    "    \n",
    "    # If record\n",
    "    if record:\n",
    "        \n",
    "        # Record the agent playing\n",
    "        env = gym.wrappers.Monitor(env, record)\n",
    "        \n",
    "    # Initialize the network\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n)\n",
    "    \n",
    "    # Load the weights\n",
    "    net.load_state_dict(torch.load(model, map_location = lambda storage, loc: storage))\n",
    "\n",
    "    # Reset the state and get the initial observation\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Initialize the total reward\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    # Initialize the counter\n",
    "    c = collections.Counter()\n",
    "\n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Start the timer\n",
    "        start_ts = time.time()\n",
    "        \n",
    "        # If visualize\n",
    "        if visualize:\n",
    "            \n",
    "            # Render the environment\n",
    "            env.render()\n",
    "            \n",
    "        # Convert state into a tensor\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        \n",
    "        # Get the Q-Values\n",
    "        # Pass forward the state into the network + Get the data + Convert it into an array + Get the first dimension\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        \n",
    "        # Get the maximum value in Q-Values\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        # Increment the action count\n",
    "        c[action] += 1\n",
    "        \n",
    "        # Take the given action and get the (state, reward, is_done, info)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Add reward into total_reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # If terminal state\n",
    "        if done:\n",
    "            \n",
    "            # Break the loop\n",
    "            break\n",
    "        \n",
    "        # If visualize\n",
    "        if visualize:\n",
    "            \n",
    "            # Calculate the delta\n",
    "            delta = (1 / FPS) - (time.time() - start_ts)\n",
    "            \n",
    "            # If delta is positive\n",
    "            if delta > 0:\n",
    "                \n",
    "                # Delay execution for delta seconds\n",
    "                time.sleep(delta)\n",
    "                \n",
    "    # Print the total reward\n",
    "    print(\"Total reward: %.2f\" % total_reward)\n",
    "    \n",
    "    # Print the action counts\n",
    "    print(\"Action counts:\", c)\n",
    "    \n",
    "    # If record\n",
    "    if record:\n",
    "        \n",
    "        # Override close in your subclass to perform any necessary cleanup\n",
    "        env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 05. Summary\n",
    "\n",
    "---\n",
    "\n",
    "In this chapter, we introduced lots of new and complex material. We became familiar with the limitations of value iteration in complex environments with large observation spaces and discussed how to overcome them with Q-learning. We checked the Q-learning algorithm on the FrozenLake environment and discussed the approximation of Q-values with neural networks and the extra complications that arise from this approximation. We covered several tricks for DQNs to improve their training stability and convergence, such as experience replay buffer, target networks, and frame stacking. Finally, we combined those extensions in to one single implementation of DQN that solves the Pong environment from the Atari games suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___THE END___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
